{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Data Extraction from PhysioNet\n",
    "\n",
    "This Jupyter Notebook downloads patient `.txt` files from the I-CARE dataset hosted on PhysioNet. Each file contains information about individual patients, such as their age, hospital, ROSC, and outcome. The goal is to download these files, extract the relevant information, and store it in a structured format like a Pandas DataFrame.\n",
    "\n",
    "This procedure is necessary, as bulk downloading the data is not working due to the sizes of the eeg and ecg data of over 1.5TB.\n",
    "\n",
    "## Steps\n",
    "1. Import libraries\n",
    "2. Set Variables\n",
    "3. Load the patient list\n",
    "4. Iterate through all patients and download the txt file\n",
    "5. Save the data as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "First, we import the necessary Python libraries for making HTTP requests, working with data, and managing file paths.\n",
    "\n",
    "- `requests`: For downloading the patient files from the web.\n",
    "- `pandas`: To store and manipulate the extracted data.\n",
    "- `os`: For handling file paths and directory creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the variables\n",
    "The variables such as the URL, patient dictionary and file pathes need to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://physionet.org/content/i-care/2.1/training/\"\n",
    "records_url = base_url + \"RECORDS\"\n",
    "patient_numbers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = \"data\"\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download the RECORDS file of patients\n",
    "\n",
    "We are firstly accessing the patient list, that is published.\n",
    "- Open and read the file html\n",
    "- parse the HTML for the patient numbers\n",
    "- create a list with the patient numbers (without the 'patient' part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the RECORDS file to get the list of patient numbers\n",
    "response = requests.get(records_url)\n",
    "if response.status_code == 200:\n",
    "    # The RECORDS file is a plain text file, so split it by lines to get the patient numbers\n",
    "    patient_numbers_html = response.text\n",
    "else:\n",
    "    print(\"Failed to download the RECORDS file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0284', '0286', '0296', '0299', '0303', '0306', '0311', '0312', '0313', '0316', '0319', '0320', '0326', '0328', '0332', '0334', '0335', '0337', '0340', '0341', '0342', '0344', '0346', '0347', '0348', '0349', '0350', '0351', '0352', '0353', '0354', '0355', '0356', '0357', '0358', '0359', '0360', '0361', '0362', '0363', '0364', '0365', '0366', '0367', '0368', '0369', '0370', '0371', '0372', '0373', '0375', '0376', '0377', '0378', '0379', '0380', '0382', '0383', '0384', '0385', '0387', '0389', '0390', '0391', '0392', '0394', '0395', '0396', '0397', '0398', '0399', '0400', '0402', '0403', '0404', '0405', '0406', '0407', '0409', '0410', '0411', '0412', '0413', '0414', '0415', '0416', '0417', '0418', '0419', '0420', '0421', '0422', '0423', '0424', '0426', '0427', '0428', '0429', '0430', '0431', '0432', '0433', '0434', '0435', '0436', '0437', '0438', '0439', '0440', '0441', '0442', '0443', '0444', '0445', '0446', '0447', '0448', '0450', '0451', '0452', '0453', '0455', '0456', '0457', '0458', '0459', '0460', '0461', '0462', '0463', '0464', '0465', '0466', '0467', '0468', '0469', '0470', '0471', '0472', '0473', '0474', '0475', '0476', '0477', '0479', '0481', '0482', '0483', '0484', '0485', '0486', '0487', '0490', '0492', '0493', '0495', '0496', '0497', '0498', '0500', '0501', '0502', '0504', '0505', '0506', '0507', '0508', '0510', '0512', '0513', '0514', '0515', '0517', '0518', '0519', '0520', '0521', '0522', '0523', '0525', '0526', '0527', '0529', '0530', '0531', '0532', '0533', '0535', '0536', '0538', '0539', '0540', '0541', '0542', '0543', '0544', '0545', '0546', '0547', '0548', '0549', '0550', '0552', '0553', '0554', '0555', '0556', '0558', '0559', '0560', '0561', '0562', '0563', '0564', '0565', '0566', '0567', '0568', '0569', '0570', '0571', '0573', '0574', '0575', '0576', '0577', '0579', '0580', '0582', '0584', '0585', '0586', '0587', '0588', '0589', '0590', '0591', '0592', '0593', '0595', '0597', '0598', '0600', '0601', '0602', '0604', '0605', '0606', '0607', '0609', '0610', '0611', '0612', '0613', '0614', '0615', '0616', '0617', '0618', '0619', '0621', '0623', '0624', '0625', '0626', '0627', '0628', '0629', '0630', '0631', '0632', '0633', '0634', '0635', '0636', '0637', '0638', '0639', '0641', '0642', '0644', '0645', '0646', '0647', '0648', '0649', '0650', '0651', '0652', '0655', '0656', '0657', '0658', '0660', '0661', '0663', '0665', '0666', '0668', '0669', '0670', '0671', '0672', '0673', '0674', '0675', '0676', '0677', '0678', '0679', '0680', '0681', '0682', '0683', '0684', '0685', '0686', '0688', '0689', '0690', '0691', '0692', '0693', '0694', '0695', '0697', '0699', '0700', '0701', '0702', '0703', '0706', '0707', '0708', '0709', '0710', '0711', '0712', '0713', '0714', '0715', '0716', '0717', '0718', '0719', '0720', '0721', '0722', '0723', '0724', '0725', '0726', '0727', '0728', '0729', '0730', '0731', '0732', '0734', '0736', '0737', '0738', '0739', '0740', '0741', '0742', '0744', '0745', '0746', '0747', '0748', '0749', '0750', '0751', '0752', '0753', '0754', '0755', '0756', '0757', '0758', '0759', '0760', '0761', '0764', '0765', '0766', '0767', '0768', '0769', '0770', '0771', '0772', '0773', '0774', '0775', '0776', '0777', '0778', '0779', '0780', '0781', '0782', '0783', '0784', '0785', '0787', '0788', '0789', '0790', '0792', '0794', '0796', '0797', '0799', '0800', '0801', '0804', '0805', '0806', '0807', '0808', '0809', '0810', '0811', '0812', '0813', '0814', '0816', '0817', '0819', '0820', '0821', '0822', '0823', '0824', '0826', '0827', '0828', '0829', '0830', '0831', '0832', '0833', '0834', '0835', '0837', '0838', '0839', '0840', '0841', '0843', '0844', '0845', '0846', '0847', '0848', '0850', '0851', '0852', '0853', '0854', '0855', '0856', '0857', '0858', '0859', '0860', '0861', '0862', '0864', '0865', '0866', '0867', '0868', '0869', '0870', '0871', '0872', '0873', '0874', '0875', '0876', '0877', '0879', '0880', '0881', '0882', '0883', '0884', '0885', '0886', '0887', '0888', '0889', '0890', '0891', '0892', '0893', '0894', '0895', '0896', '0897', '0898', '0899', '0900', '0901', '0902', '0903', '0904', '0905', '0907', '0908', '0909', '0910', '0911', '0913', '0915', '0916', '0917', '0918', '0919', '0920', '0921', '0922', '0923', '0924', '0925', '0926', '0928', '0929', '0930', '0931', '0932', '0933', '0934', '0935', '0937', '0941', '0942', '0943', '0944', '0945', '0947', '0948', '0950', '0951', '0952', '0953', '0954', '0955', '0957', '0958', '0960', '0961', '0962', '0963', '0964', '0965', '0966', '0967', '0968', '0969', '0970', '0971', '0973', '0974', '0975', '0976', '0977', '0978', '0979', '0980', '0981', '0982', '0984', '0985', '0987', '0988', '0989', '0991', '0993', '0994', '0996', '0997', '0998', '0999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1011', '1012', '1013', '1014', '1015', '1016', '1017', '1018', '1019', '1020']\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(patient_numbers_html, 'html.parser')\n",
    "\n",
    "# Find the <pre> tag with class \"plain\" and the <code> tag within it\n",
    "code_tag = soup.find('pre', {'class': 'plain'}).find('code')\n",
    "\n",
    "# Extract the content of the <code> tag\n",
    "patient_records = code_tag.text if code_tag else ''\n",
    "\n",
    "# Split the content into individual records (one per line)\n",
    "patient_records_list = patient_records.splitlines()\n",
    "\n",
    "# Clean the list by removing any empty strings\n",
    "patient_records_list = [record.strip() for record in patient_records_list if record.strip()]\n",
    "\n",
    "# Clean up the patient numbers by removing 'training/' and the trailing '/'\n",
    "patient_numbers = [record.split('/')[1] for record in patient_records_list]\n",
    "\n",
    "# Print the cleaned patient numbers to verify\n",
    "print(patient_numbers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download each patient file and get the information\n",
    "\n",
    "We are now taking the list of the patients, build the links to the txt files that hold the patient information data such as number, hospital, age, sex, rosc, ohca, rhythm, ttm, outcome and cpc.\n",
    "\n",
    "- Build the link\n",
    "- Access the file and HTML\n",
    "- Parse the HTML for the needed part\n",
    "- Create a dictionary from the parsed HTML\n",
    "- Create a df and add it to the overall df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame with the expected columns\n",
    "columns = ['Patient', 'Hospital', 'Age', 'Sex', 'ROSC', 'OHCA', 'Shockable Rhythm', 'TTM', 'Outcome', 'CPC']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Download each patient's .txt file\n",
    "for patient_number in patient_numbers:\n",
    "    # Construct the full URL to the .txt file\n",
    "    file_url = f\"{base_url}/{patient_number}/{patient_number}.txt\"\n",
    "\n",
    "    # Send the HTTP request to download the file\n",
    "    response = requests.get(file_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the <pre> tag with class \"plain\" and the <code> tag within it\n",
    "        code_tag = soup.find('pre', {'class': 'plain'}).find('code')\n",
    "\n",
    "        # Extract the content of the <code> tag\n",
    "        patient_data = code_tag.text if code_tag else ''\n",
    "\n",
    "        # Split the content into individual records (one per line)\n",
    "        patient_data_list = patient_data.splitlines()\n",
    "\n",
    "        # Clean the list by removing any empty strings\n",
    "        patient_data_list = [record.strip() for record in patient_data_list if record.strip()]\n",
    "\n",
    "        # Step 1: Create a dictionary from the list by splitting each string on \": \"\n",
    "        patient_dict = {item.split(\": \")[0]: item.split(\": \")[1] for item in patient_data_list}\n",
    "\n",
    "        # Step 2: Convert the dictionary into a DataFrame\n",
    "        patient_df = pd.DataFrame([patient_dict])\n",
    "        \n",
    "        # Step 3: Concatenate the new row to the existing DataFrame\n",
    "        df = pd.concat([df, patient_df], ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to download file for patient {patient_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save the Dataframe to a csv file\n",
    "Now save the collected data to a csv file that makes it accessible in other data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to 'data/patient_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('data/raw_patient_data.csv', index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(\"DataFrame has been saved to 'data/raw_patient_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
