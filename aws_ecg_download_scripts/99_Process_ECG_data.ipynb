{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "from wfdb import processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Use the signal for filters\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from scipy.signal import welch, medfilt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to download the patient overview\n",
    "base_url = \"https://physionet.org/content/i-care/2.1/training/\"\n",
    "records_url = base_url + \"RECORDS\"\n",
    "\n",
    "# Base directory path for the PhysioNet database\n",
    "physionet_db_path = \"i-care/2.1/training\"\n",
    "\n",
    "patient_numbers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = \"data\"\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the RECORDS file to get the list of patient numbers\n",
    "response = requests.get(records_url)\n",
    "if response.status_code == 200:\n",
    "    # The RECORDS file is a plain text file, so split it by lines to get the patient numbers\n",
    "    patient_numbers_html = response.text\n",
    "else:\n",
    "    print(\"Failed to download the RECORDS file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0284', '0286', '0296', '0299', '0303', '0306', '0311', '0312', '0313', '0316', '0319', '0320', '0326', '0328', '0332', '0334', '0335', '0337', '0340', '0341', '0342', '0344', '0346', '0347', '0348', '0349', '0350', '0351', '0352', '0353', '0354', '0355', '0356', '0357', '0358', '0359', '0360', '0361', '0362', '0363', '0364', '0365', '0366', '0367', '0368', '0369', '0370', '0371', '0372', '0373', '0375', '0376', '0377', '0378', '0379', '0380', '0382', '0383', '0384', '0385', '0387', '0389', '0390', '0391', '0392', '0394', '0395', '0396', '0397', '0398', '0399', '0400', '0402', '0403', '0404', '0405', '0406', '0407', '0409', '0410', '0411', '0412', '0413', '0414', '0415', '0416', '0417', '0418', '0419', '0420', '0421', '0422', '0423', '0424', '0426', '0427', '0428', '0429', '0430', '0431', '0432', '0433', '0434', '0435', '0436', '0437', '0438', '0439', '0440', '0441', '0442', '0443', '0444', '0445', '0446', '0447', '0448', '0450', '0451', '0452', '0453', '0455', '0456', '0457', '0458', '0459', '0460', '0461', '0462', '0463', '0464', '0465', '0466', '0467', '0468', '0469', '0470', '0471', '0472', '0473', '0474', '0475', '0476', '0477', '0479', '0481', '0482', '0483', '0484', '0485', '0486', '0487', '0490', '0492', '0493', '0495', '0496', '0497', '0498', '0500', '0501', '0502', '0504', '0505', '0506', '0507', '0508', '0510', '0512', '0513', '0514', '0515', '0517', '0518', '0519', '0520', '0521', '0522', '0523', '0525', '0526', '0527', '0529', '0530', '0531', '0532', '0533', '0535', '0536', '0538', '0539', '0540', '0541', '0542', '0543', '0544', '0545', '0546', '0547', '0548', '0549', '0550', '0552', '0553', '0554', '0555', '0556', '0558', '0559', '0560', '0561', '0562', '0563', '0564', '0565', '0566', '0567', '0568', '0569', '0570', '0571', '0573', '0574', '0575', '0576', '0577', '0579', '0580', '0582', '0584', '0585', '0586', '0587', '0588', '0589', '0590', '0591', '0592', '0593', '0595', '0597', '0598', '0600', '0601', '0602', '0604', '0605', '0606', '0607', '0609', '0610', '0611', '0612', '0613', '0614', '0615', '0616', '0617', '0618', '0619', '0621', '0623', '0624', '0625', '0626', '0627', '0628', '0629', '0630', '0631', '0632', '0633', '0634', '0635', '0636', '0637', '0638', '0639', '0641', '0642', '0644', '0645', '0646', '0647', '0648', '0649', '0650', '0651', '0652', '0655', '0656', '0657', '0658', '0660', '0661', '0663', '0665', '0666', '0668', '0669', '0670', '0671', '0672', '0673', '0674', '0675', '0676', '0677', '0678', '0679', '0680', '0681', '0682', '0683', '0684', '0685', '0686', '0688', '0689', '0690', '0691', '0692', '0693', '0694', '0695', '0697', '0699', '0700', '0701', '0702', '0703', '0706', '0707', '0708', '0709', '0710', '0711', '0712', '0713', '0714', '0715', '0716', '0717', '0718', '0719', '0720', '0721', '0722', '0723', '0724', '0725', '0726', '0727', '0728', '0729', '0730', '0731', '0732', '0734', '0736', '0737', '0738', '0739', '0740', '0741', '0742', '0744', '0745', '0746', '0747', '0748', '0749', '0750', '0751', '0752', '0753', '0754', '0755', '0756', '0757', '0758', '0759', '0760', '0761', '0764', '0765', '0766', '0767', '0768', '0769', '0770', '0771', '0772', '0773', '0774', '0775', '0776', '0777', '0778', '0779', '0780', '0781', '0782', '0783', '0784', '0785', '0787', '0788', '0789', '0790', '0792', '0794', '0796', '0797', '0799', '0800', '0801', '0804', '0805', '0806', '0807', '0808', '0809', '0810', '0811', '0812', '0813', '0814', '0816', '0817', '0819', '0820', '0821', '0822', '0823', '0824', '0826', '0827', '0828', '0829', '0830', '0831', '0832', '0833', '0834', '0835', '0837', '0838', '0839', '0840', '0841', '0843', '0844', '0845', '0846', '0847', '0848', '0850', '0851', '0852', '0853', '0854', '0855', '0856', '0857', '0858', '0859', '0860', '0861', '0862', '0864', '0865', '0866', '0867', '0868', '0869', '0870', '0871', '0872', '0873', '0874', '0875', '0876', '0877', '0879', '0880', '0881', '0882', '0883', '0884', '0885', '0886', '0887', '0888', '0889', '0890', '0891', '0892', '0893', '0894', '0895', '0896', '0897', '0898', '0899', '0900', '0901', '0902', '0903', '0904', '0905', '0907', '0908', '0909', '0910', '0911', '0913', '0915', '0916', '0917', '0918', '0919', '0920', '0921', '0922', '0923', '0924', '0925', '0926', '0928', '0929', '0930', '0931', '0932', '0933', '0934', '0935', '0937', '0941', '0942', '0943', '0944', '0945', '0947', '0948', '0950', '0951', '0952', '0953', '0954', '0955', '0957', '0958', '0960', '0961', '0962', '0963', '0964', '0965', '0966', '0967', '0968', '0969', '0970', '0971', '0973', '0974', '0975', '0976', '0977', '0978', '0979', '0980', '0981', '0982', '0984', '0985', '0987', '0988', '0989', '0991', '0993', '0994', '0996', '0997', '0998', '0999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1011', '1012', '1013', '1014', '1015', '1016', '1017', '1018', '1019', '1020']\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(patient_numbers_html, 'html.parser')\n",
    "\n",
    "# Find the <pre> tag with class \"plain\" and the <code> tag within it\n",
    "code_tag = soup.find('pre', {'class': 'plain'}).find('code')\n",
    "\n",
    "# Extract the content of the <code> tag\n",
    "patient_records = code_tag.text if code_tag else ''\n",
    "\n",
    "# Split the content into individual records (one per line)\n",
    "patient_records_list = patient_records.splitlines()\n",
    "\n",
    "# Clean the list by removing any empty strings\n",
    "patient_records_list = [record.strip() for record in patient_records_list if record.strip()]\n",
    "\n",
    "# Clean up the patient numbers by removing 'training/' and the trailing '/'\n",
    "patient_numbers = [record.split('/')[1] for record in patient_records_list]\n",
    "\n",
    "# Print the cleaned patient numbers to verify\n",
    "print(patient_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Patient_ID, Segment_1_Mean_HR, Segment_1_HRV_SDNN, Segment_1_LF_Power, Segment_1_HF_Power, Segment_1_LF_HF_Ratio, Segment_2_Mean_HR, Segment_2_HRV_SDNN, Segment_2_LF_Power, Segment_2_HF_Power, Segment_2_LF_HF_Ratio, Segment_3_Mean_HR, Segment_3_HRV_SDNN, Segment_3_LF_Power, Segment_3_HF_Power, Segment_3_LF_HF_Ratio, Segment_4_Mean_HR, Segment_4_HRV_SDNN, Segment_4_LF_Power, Segment_4_HF_Power, Segment_4_LF_HF_Ratio, Segment_5_Mean_HR, Segment_5_HRV_SDNN, Segment_5_LF_Power, Segment_5_HF_Power, Segment_5_LF_HF_Ratio, Segment_6_Mean_HR, Segment_6_HRV_SDNN, Segment_6_LF_Power, Segment_6_HF_Power, Segment_6_LF_HF_Ratio, Segment_7_Mean_HR, Segment_7_HRV_SDNN, Segment_7_LF_Power, Segment_7_HF_Power, Segment_7_LF_HF_Ratio, Segment_8_Mean_HR, Segment_8_HRV_SDNN, Segment_8_LF_Power, Segment_8_HF_Power, Segment_8_LF_HF_Ratio, Segment_9_Mean_HR, Segment_9_HRV_SDNN, Segment_9_LF_Power, Segment_9_HF_Power, Segment_9_LF_HF_Ratio, Segment_10_Mean_HR, Segment_10_HRV_SDNN, Segment_10_LF_Power, Segment_10_HF_Power, Segment_10_LF_HF_Ratio, Segment_11_Mean_HR, Segment_11_HRV_SDNN, Segment_11_LF_Power, Segment_11_HF_Power, Segment_11_LF_HF_Ratio, Segment_12_Mean_HR, Segment_12_HRV_SDNN, Segment_12_LF_Power, Segment_12_HF_Power, Segment_12_LF_HF_Ratio, Segment_13_Mean_HR, Segment_13_HRV_SDNN, Segment_13_LF_Power, Segment_13_HF_Power, Segment_13_LF_HF_Ratio, Segment_14_Mean_HR, Segment_14_HRV_SDNN, Segment_14_LF_Power, Segment_14_HF_Power, Segment_14_LF_HF_Ratio, Segment_15_Mean_HR, Segment_15_HRV_SDNN, Segment_15_LF_Power, Segment_15_HF_Power, Segment_15_LF_HF_Ratio, Segment_16_Mean_HR, Segment_16_HRV_SDNN, Segment_16_LF_Power, Segment_16_HF_Power, Segment_16_LF_HF_Ratio, Segment_17_Mean_HR, Segment_17_HRV_SDNN, Segment_17_LF_Power, Segment_17_HF_Power, Segment_17_LF_HF_Ratio, Segment_18_Mean_HR, Segment_18_HRV_SDNN, Segment_18_LF_Power, Segment_18_HF_Power, Segment_18_LF_HF_Ratio, Segment_19_Mean_HR, Segment_19_HRV_SDNN, Segment_19_LF_Power, Segment_19_HF_Power, Segment_19_LF_HF_Ratio, Segment_20_Mean_HR, Segment_20_HRV_SDNN, Segment_20_LF_Power, Segment_20_HF_Power, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 1321 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the total number of segments based on `hours_included` in 5-minute intervals\n",
    "hours_included = 22\n",
    "total_segments = (hours_included * 60) // 5\n",
    "\n",
    "# Define the columns by including `Mean_HR`, `HRV_SDNN`, `LF_Power`, `HF_Power`, and `LF/HF_Ratio` for each segment\n",
    "columns = [\"Patient_ID\"] + [\n",
    "    f\"Segment_{i+1}_Mean_HR\" if j == 0 else \n",
    "    f\"Segment_{i+1}_HRV_SDNN\" if j == 1 else \n",
    "    f\"Segment_{i+1}_LF_Power\" if j == 2 else \n",
    "    f\"Segment_{i+1}_HF_Power\" if j == 3 else \n",
    "    f\"Segment_{i+1}_LF_HF_Ratio\"\n",
    "    for i in range(total_segments) for j in range(5)\n",
    "]\n",
    "\n",
    "# Create the DataFrame with specified columns\n",
    "final_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Check the DataFrame structure\n",
    "print(final_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_high_low_filter(signal, sampling_rate, utility_freq):\n",
    "    # Step 1: High-pass filter to remove baseline drift (1 Hz cutoff)\n",
    "    b_high, a_high = butter(2, 1 / (sampling_rate / 2), btype='high')\n",
    "    filtered_signal = filtfilt(b_high, a_high, signal)\n",
    "\n",
    "    # Step 2: Notch filter to remove utility frequency noise (50 Hz or 60 Hz)\n",
    "    Q = 30.0  # Quality factor for notch filter\n",
    "    notch_freq = utility_freq / (sampling_rate / 2)\n",
    "    b_notch, a_notch = iirnotch(notch_freq, Q)\n",
    "    filtered_signal = filtfilt(b_notch, a_notch, filtered_signal)\n",
    "\n",
    "    # Step 3: Stronger low-pass filter to reduce high-frequency noise (40 Hz cutoff, 4th order)\n",
    "    b_low, a_low = butter(4, 40 / (sampling_rate / 2), btype='low')\n",
    "    filtered_signal = filtfilt(b_low, a_low, filtered_signal)\n",
    "\n",
    "    # Step 4: Apply a median filter to remove spikes\n",
    "    filtered_signal = medfilt(filtered_signal, kernel_size=5)\n",
    "\n",
    "    return filtered_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from wfdb import processing\n",
    "\n",
    "def calculate_hr_hrv_lf_hf_ratio(signal, sampling_rate, local_start_time, total_samples):\n",
    "    print(f\"Sampling: {sampling_rate}\")\n",
    "    # Set thresholds and frequency bands\n",
    "    hr_threshold = (30, 200)\n",
    "    hrv_threshold = (0, 300)\n",
    "    lf_band = (0.04, 0.15)\n",
    "    hf_band = (0.15, 0.4)\n",
    "\n",
    "    # Calculate the duration to the next full 5-minute boundary from local_start_time\n",
    "    first_segment_duration = timedelta(minutes=5) - timedelta(seconds=local_start_time.total_seconds() % 300)\n",
    "    first_segment_duration_seconds = first_segment_duration.total_seconds()\n",
    "\n",
    "    # Calculate the sample index to end the first segment\n",
    "    first_segment_end_index = int(first_segment_duration_seconds * sampling_rate)\n",
    "    first_segment_signal = signal[:first_segment_end_index]\n",
    "\n",
    "    # Initialize XQRS for the first segment and detect QRS complexes\n",
    "    xqrs = processing.XQRS(sig=first_segment_signal, fs=sampling_rate)\n",
    "    xqrs.detect(learn=False, verbose=False)\n",
    "    qrs_indices = np.array(xqrs.qrs_inds)  # Indices of detected R-peaks in sample points\n",
    "    r_wave_times = qrs_indices / sampling_rate\n",
    "\n",
    "    # Initialize lists to store HR, HRV, LF, HF values, LF/HF ratios, and QRS indices\n",
    "    hr_values, hrv_values, lf_powers, hf_powers, lfhf_ratios = [], [], [], [], []\n",
    "    all_qrs_indices = []  # List to accumulate QRS indices from each segment\n",
    "    segment_start_indices = [0]\n",
    "\n",
    "    # Append QRS indices for the first segment\n",
    "    all_qrs_indices.extend(qrs_indices)\n",
    "\n",
    "    # Process the shortened first segment\n",
    "    first_segment_indices = r_wave_times < first_segment_duration_seconds\n",
    "    first_segment_rr_intervals = np.diff(r_wave_times[first_segment_indices]) * 1000  # Convert to milliseconds\n",
    "\n",
    "    # Calculate HR, HRV, LF, HF, and LF/HF for the first segment\n",
    "    if len(first_segment_rr_intervals) > 0:\n",
    "        mean_hr = int(round(processing.calc_mean_hr(first_segment_rr_intervals / 1000, rr_units=\"seconds\")))\n",
    "        hrv = int(round(np.std(first_segment_rr_intervals)))\n",
    "\n",
    "        # Apply threshold conditions\n",
    "        mean_hr = mean_hr if hr_threshold[0] <= mean_hr <= hr_threshold[1] else np.nan\n",
    "        hrv = hrv if hrv_threshold[0] <= hrv <= hrv_threshold[1] else np.nan\n",
    "\n",
    "        # Calculate LF and HF power\n",
    "        freqs, psd = welch(first_segment_rr_intervals, fs=1.0, nperseg=len(first_segment_rr_intervals))\n",
    "        lf_power = int(np.trapezoid(psd[(freqs >= lf_band[0]) & (freqs < lf_band[1])], freqs[(freqs >= lf_band[0]) & (freqs < lf_band[1])]))\n",
    "        hf_power = int(np.trapezoid(psd[(freqs >= hf_band[0]) & (freqs < hf_band[1])], freqs[(freqs >= hf_band[0]) & (freqs < hf_band[1])]))\n",
    "        lfhf_ratio = round((lf_power / hf_power), 2) if hf_power > 0 else np.nan\n",
    "    else:\n",
    "        mean_hr, hrv, lf_power, hf_power, lfhf_ratio = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Append results for the first segment\n",
    "    hr_values.append(mean_hr)\n",
    "    hrv_values.append(hrv)\n",
    "    lf_powers.append(lf_power)\n",
    "    hf_powers.append(hf_power)\n",
    "    lfhf_ratios.append(lfhf_ratio)\n",
    "\n",
    "    # Process remaining full 5-minute segments\n",
    "    current_time = first_segment_duration_seconds\n",
    "    num_segments = int((total_samples / sampling_rate - current_time) // 300)  # Remaining time for full segments\n",
    "    for _ in range(num_segments):\n",
    "        segment_start_index = int(current_time * sampling_rate)\n",
    "        segment_start_indices.append(segment_start_index)\n",
    "        \n",
    "        segment_end = current_time + 300\n",
    "        segment_signal = signal[segment_start_index:int(segment_end * sampling_rate)]\n",
    "        \n",
    "        # Initialize XQRS for the current segment\n",
    "        xqrs = processing.XQRS(sig=segment_signal, fs=sampling_rate)\n",
    "        xqrs.detect(learn=False, verbose=False)\n",
    "        qrs_indices = np.array(xqrs.qrs_inds)\n",
    "        all_qrs_indices.extend(qrs_indices + segment_start_index)  # Adjust indices to the full signal context\n",
    "        selected_r_wave_times = (qrs_indices + segment_start_index) / sampling_rate  # Convert to seconds with offset\n",
    "        segment_rr_intervals = np.diff(selected_r_wave_times) * 1000  # Convert RR intervals to milliseconds\n",
    "\n",
    "        # Calculate HR, HRV, LF, HF, and LF/HF for the segment\n",
    "        if len(segment_rr_intervals) > 0:\n",
    "            mean_hr = int(round(processing.calc_mean_hr(segment_rr_intervals / 1000, rr_units=\"seconds\")))\n",
    "            hrv = int(round(np.std(segment_rr_intervals)))\n",
    "\n",
    "            mean_hr = mean_hr if hr_threshold[0] <= mean_hr <= hr_threshold[1] else np.nan\n",
    "            hrv = hrv if hrv_threshold[0] <= hrv <= hrv_threshold[1] else np.nan\n",
    "\n",
    "            freqs, psd = welch(segment_rr_intervals, fs=1.0, nperseg=len(segment_rr_intervals))\n",
    "            lf_power = int(np.trapezoid(psd[(freqs >= lf_band[0]) & (freqs < lf_band[1])], freqs[(freqs >= lf_band[0]) & (freqs < lf_band[1])]))\n",
    "            hf_power = int(np.trapezoid(psd[(freqs >= hf_band[0]) & (freqs < hf_band[1])], freqs[(freqs >= hf_band[0]) & (freqs < hf_band[1])]))\n",
    "            lfhf_ratio = round((lf_power / hf_power), 2) if hf_power > 0 else np.nan\n",
    "        else:\n",
    "            mean_hr, hrv, lf_power, hf_power, lfhf_ratio = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append the results for each segment\n",
    "        hr_values.append(mean_hr)\n",
    "        hrv_values.append(hrv)\n",
    "        lf_powers.append(lf_power)\n",
    "        hf_powers.append(hf_power)\n",
    "        lfhf_ratios.append(lfhf_ratio)\n",
    "\n",
    "        # Move to the next 5-minute segment\n",
    "        current_time = segment_end\n",
    "\n",
    "    # Convert all_qrs_indices to a numpy array if needed\n",
    "    all_qrs_indices = np.array(all_qrs_indices)\n",
    "\n",
    "    return hr_values, hrv_values, lf_powers, hf_powers, lfhf_ratios, all_qrs_indices, segment_start_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_final_structure(patient_id, hr_hrv_df, final_df):\n",
    "    # Create an empty row with NaN values for all segments\n",
    "    row_data = {\"Patient_ID\": patient_id}\n",
    "    for column in final_df.columns:\n",
    "        if column != \"Patient_ID\":\n",
    "            row_data[column] = np.nan\n",
    "\n",
    "    # If hr_hrv_df has data, fill in the corresponding HR, HRV, LF, HF, and LF/HF values\n",
    "    if not hr_hrv_df.empty:\n",
    "        for i in range(len(hr_hrv_df)):\n",
    "            hr_column = f\"Segment_{i+1}_Mean_HR\"\n",
    "            hrv_column = f\"Segment_{i+1}_HRV_SDNN\"\n",
    "            lf_column = f\"Segment_{i+1}_LF_Power\"\n",
    "            hf_column = f\"Segment_{i+1}_HF_Power\"\n",
    "            lfhf_ratio_column = f\"Segment_{i+1}_LF_HF_Ratio\"\n",
    "            \n",
    "            # Assign values using column names that match hr_hrv_df\n",
    "            row_data[hr_column] = hr_hrv_df.at[i, \"Mean HR (bpm)\"]\n",
    "            row_data[hrv_column] = hr_hrv_df.at[i, \"HRV (SDNN) (ms)\"]\n",
    "            row_data[lf_column] = hr_hrv_df.at[i, \"LF Power (ms^2)\"]\n",
    "            row_data[hf_column] = hr_hrv_df.at[i, \"HF Power (ms^2)\"]\n",
    "            row_data[lfhf_ratio_column] = hr_hrv_df.at[i, \"LF/HF Ratio\"]\n",
    "\n",
    "    # Convert row_data dictionary to a DataFrame and concatenate with final_df\n",
    "    new_row_df = pd.DataFrame([row_data])\n",
    "    final_df = pd.concat([final_df, new_row_df], ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_records(patient_number):\n",
    "    # Construct the full URL to the RECORDS file\n",
    "    file_url = f\"{base_url}/{patient_number}/RECORDS\"\n",
    "\n",
    "    # Fetch the URL content\n",
    "    response = requests.get(file_url)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the text content of the RECORDS file\n",
    "        patient_records = soup.text  # Plain text response for RECORDS\n",
    "\n",
    "        # Split the content into lines and filter for `_ECG` files with last 3 digits <= 023\n",
    "        ecg_files = [\n",
    "            line for line in patient_records.splitlines()\n",
    "            if line.endswith(\"_ECG\") and int(line.split(\"_\")[-2]) <= 5\n",
    "        ]\n",
    "\n",
    "        return ecg_files\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "Starting Patient 1 of 1 (Patient number 0432)\n",
      "===================================================\n",
      "Successfully loaded: 0432_001_005_ECG\n",
      "Loaded second lead.\n",
      "hr_values: [87, 184, nan, 194, 200, 105]\n",
      "hrv_values: [257, 21, 58, 63, 59, nan]\n",
      "lf_values: [165, 0, 0, 0, 0, 4574]\n",
      "hf_values: [19876, 0, 2079, 3929, 2079, 6959]\n",
      "lfhf_ratios: [0.01, nan, 0.0, 0.0, 0.0, 0.66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kentf\\AppData\\Local\\Temp\\ipykernel_7596\\744392416.py:26: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first 20 entries\n",
    "first_20_patients = ['0432'] # patient_numbers[:2]\n",
    "\n",
    "total_patients = len(first_20_patients)\n",
    "\n",
    "for i, patient_number in enumerate(first_20_patients, start=1):\n",
    "    print(\"===================================================\")\n",
    "    print(f\"Starting Patient {i} of {total_patients} (Patient number {patient_number})\")\n",
    "    print(\"===================================================\")\n",
    "    ecg_files = get_patient_records(patient_number)\n",
    "\n",
    "\n",
    "    # Initialize a DataFrame for a full 24-hour period, with 5-minute segments\n",
    "    total_duration_hours = 24\n",
    "    segment_duration = timedelta(minutes=5)\n",
    "    total_segments = (total_duration_hours * 60) // 5\n",
    "\n",
    "    # Create the full DataFrame with NaN values initially\n",
    "    hr_hrv_df = pd.DataFrame({\n",
    "        \"Start Time\": [timedelta(minutes=5 * i) for i in range(total_segments)],\n",
    "        \"Mean HR (bpm)\": [np.nan] * total_segments,\n",
    "        \"HRV (SDNN) (ms)\": [np.nan] * total_segments\n",
    "    })\n",
    "\n",
    "    for ecg_file in ecg_files:\n",
    "\n",
    "        # Construct the file's record name (without extension)\n",
    "        record_name = ecg_file\n",
    "        \n",
    "        try:\n",
    "            # Load the record using PhysioNet-specific directory\n",
    "            record = wfdb.rdrecord(record_name, pn_dir=f\"{physionet_db_path}/{patient_number}\")\n",
    "            print(f\"Successfully loaded: {ecg_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {ecg_file}: {e}\")\n",
    "\n",
    "        # Get metadata of the signal\n",
    "        patient_number = record.record_name.split(\"_\")[0]\n",
    "        sampling_rate = record.fs\n",
    "        signal_len = record.sig_len\n",
    "        utility_freq = int(record.comments[0].split(\": \")[1])\n",
    "        start_time = pd.to_timedelta(record.comments[1].split(\": \")[1])\n",
    "        end_time = pd.to_timedelta(record.comments[2].split(\": \")[1])\n",
    "\n",
    "        print(sampling_rate)\n",
    "\n",
    "\n",
    "        # Access the signal data (numpy array) for the second lead\n",
    "        if record.p_signal.shape[1] > 1:  # Check if there are multiple leads\n",
    "            ecg_signal_data = record.p_signal[:, 0].flatten()  # Select the second lead\n",
    "            print(\"Loaded second lead.\")\n",
    "        else:\n",
    "            print(\"Only one lead available in this record.\")\n",
    "            # Access the signal data (numpy array)\n",
    "            ecg_signal_data = record.p_signal.flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        filtered_signal = apply_high_low_filter(ecg_signal_data, sampling_rate, utility_freq)\n",
    "\n",
    "        # Call the updated function to calculate HR, HRV, LF, HF, and LF/HF ratio\n",
    "        hr_values, hrv_values, lf_values, hf_values, lfhf_ratios, qrs_indices, segment_start_points = calculate_hr_hrv_lf_hf_ratio(\n",
    "            filtered_signal, sampling_rate, start_time, signal_len\n",
    "        )\n",
    "        print(f\"hr_values: {hr_values}\")\n",
    "        print(f\"hrv_values: {hrv_values}\")\n",
    "        print(f\"lf_values: {lf_values}\")\n",
    "        print(f\"hf_values: {hf_values}\")\n",
    "        print(f\"lfhf_ratios: {lfhf_ratios}\")\n",
    "        # Find the nearest 5-minute interval for `start_time`\n",
    "        start_index = hr_hrv_df[hr_hrv_df[\"Start Time\"] <= start_time].last_valid_index()\n",
    "\n",
    "        # Populate `hr_hrv_df` starting from `start_index`\n",
    "        for i, (hr, hrv, lf, hf, lfhf) in enumerate(zip(hr_values, hrv_values, lf_values, hf_values, lfhf_ratios)):\n",
    "            if start_index + i < len(hr_hrv_df):  # Ensure we don't go out of bounds\n",
    "                hr_hrv_df.at[start_index + i, \"Mean HR (bpm)\"] = hr\n",
    "                hr_hrv_df.at[start_index + i, \"HRV (SDNN) (ms)\"] = hrv\n",
    "                hr_hrv_df.at[start_index + i, \"LF Power (ms^2)\"] = lf\n",
    "                hr_hrv_df.at[start_index + i, \"HF Power (ms^2)\"] = hf\n",
    "                hr_hrv_df.at[start_index + i, \"LF/HF Ratio\"] = lfhf\n",
    "\n",
    "\n",
    "    # After processing all files for a patient, call this function\n",
    "    final_df = transfer_to_final_structure(patient_number, hr_hrv_df, final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scrollable_ecg_with_r_peaks_segments.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Create the Plotly plot for the ECG Signal\n",
    "trace_signal = go.Scatter(\n",
    "    y=filtered_signal,\n",
    "    mode='lines',\n",
    "    name='ECG Signal'\n",
    ")\n",
    "\n",
    "# Create the Plotly plot for the R-Peaks\n",
    "r_peaks_y = [filtered_signal[i] for i in qrs_indices]  # Get the amplitudes of the R-peaks\n",
    "trace_r_peaks = go.Scatter(\n",
    "    x=qrs_indices,\n",
    "    y=r_peaks_y,\n",
    "    mode='markers',\n",
    "    name='R-Peaks',\n",
    "    marker=dict(color='red', size=8, symbol='x')  # Customize the appearance\n",
    ")\n",
    "\n",
    "# Create vertical line markers at the start of each 5-minute segment\n",
    "segment_lines = [\n",
    "    go.layout.Shape(\n",
    "        type=\"line\",\n",
    "        x0=boundary,\n",
    "        y0=min(filtered_signal),  # Start from minimum signal amplitude\n",
    "        x1=boundary,\n",
    "        y1=max(filtered_signal),  # End at maximum signal amplitude\n",
    "        line=dict(color=\"blue\", width=1, dash=\"dash\"),\n",
    "    ) for boundary in segment_start_points\n",
    "]\n",
    "\n",
    "# Define the layout with added shapes\n",
    "layout = go.Layout(\n",
    "    title='Scrollable ECG Signal with R-Peaks and 5-Minute Segment Lines',\n",
    "    xaxis=dict(\n",
    "        title='Time (samples)',\n",
    "        rangeslider=dict(visible=True),  # Add a range slider to make it scrollable\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Amplitude',\n",
    "    ),\n",
    "    shapes=segment_lines,  # Use segment_lines instead of shapes\n",
    "    width=1200,  # Set a wider plot\n",
    "    height=500\n",
    ")\n",
    "\n",
    "# Create the figure and add both traces\n",
    "fig = go.Figure(data=[trace_signal, trace_r_peaks], layout=layout)\n",
    "\n",
    "# Show the plot in an interactive window\n",
    "pyo.plot(fig, filename='scrollable_ecg_with_r_peaks_segments.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('data/hr_hrv_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment boundaries (in samples): [0, 31744, 108544, 185344, 262144, 338944]\n"
     ]
    }
   ],
   "source": [
    "print(\"Segment boundaries (in samples):\", segment_start_points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ttm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
